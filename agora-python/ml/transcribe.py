#!/usr/bin/env python3
"""
myInterviewBot - Production v4.0 (FULLY OPTIMIZED)
‚úÖ Batch Size 32 (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–ª—è RTX 5080)
‚úÖ Pinned Memory –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è CPU‚ÜíGPU
‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ GPU/CPU
‚úÖ CUDA Streams –∏ non-blocking transfer
‚úÖ –£–º–Ω–æ–µ —Ä–∞–∑—Ä–µ–∑–∞–Ω–∏–µ –ø–æ –ø–∞—É–∑–∞–º (–∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω)

–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
  Baseline (batch16):     9.06s –Ω–∞ 1000s –∞—É–¥–∏–æ
  v4.0 OPTIMIZED:         7.33s –Ω–∞ 1000s –∞—É–¥–∏–æ
  –£—Å–∫–æ—Ä–µ–Ω–∏–µ:              1.24x

  –ù–∞ 12 —á–∞—Å–æ–≤ –∞—É–¥–∏–æ:
    –ë—ã–ª–æ: ~98 —Å–µ–∫ (1.6 –º–∏–Ω—É—Ç—ã)
    –°—Ç–∞–ª–æ: ~79 —Å–µ–∫ (1.3 –º–∏–Ω—É—Ç—ã) ‚ö°
"""

import os
os.environ['HF_HUB_OFFLINE'] = '1'

from transformers import AutoModelForCausalLM, AutoTokenizer, AutoFeatureExtractor
import torch
import librosa
import numpy as np
import time
import threading
from queue import Queue
from pathlib import Path
from datetime import datetime

print("=" * 90)
print("myInterviewBot - Production v4.0 (FULLY OPTIMIZED)")
print("=" * 90)

# ============================================
# –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ============================================
print("\n[INIT] –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å Borealis...")
model = AutoModelForCausalLM.from_pretrained(
    "Vikhrmodels/Borealis",
    trust_remote_code=True,
    local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained("Vikhrmodels/Borealis", local_files_only=True)
extractor = AutoFeatureExtractor.from_pretrained("Vikhrmodels/Borealis", local_files_only=True)

model.eval()
model.to("cuda")
model = torch.compile(model, mode="reduce-overhead", fullgraph=False)

print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {next(model.parameters()).device}")

# CUDA streams
compute_stream = torch.cuda.default_stream()
transfer_stream = torch.cuda.Stream()

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
generation_params = {
    "max_new_tokens": 350,
    "do_sample": True,
    "top_p": 0.9,
    "top_k": 50,
    "temperature": 0.2,
    "use_cache": True,
}

# ============================================
# –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò
# ============================================

def find_optimal_cut_points(waveform, sr, target_chunk_duration=30, window_duration=5):
    """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä–µ–∑–∞–Ω–∏—è –ø–æ —ç–Ω–µ—Ä–≥–∏–∏"""
    print("\n[–ê–ù–ê–õ–ò–ó] –ò—â—É –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ —Ä–∞–∑—Ä–µ–∑–∞–Ω–∏—è...")

    frame_length = 2048
    hop_length = 512

    S = librosa.stft(waveform, n_fft=frame_length, hop_length=hop_length)
    magnitude = np.abs(S)
    energy = np.sum(magnitude ** 2, axis=0)
    energy = (energy - np.min(energy)) / (np.max(energy) - np.min(energy) + 1e-10)

    def frame_to_time(frame_idx):
        return (frame_idx * hop_length) / sr

    def time_to_frame(time_s):
        return int((time_s * sr) / hop_length)

    total_duration = len(waveform) / sr
    cut_points = []

    for i in range(1, int(total_duration / target_chunk_duration) + 1):
        target_time = i * target_chunk_duration
        if target_time >= total_duration:
            break

        window_frames = int((window_duration * sr) / hop_length)
        target_frame = time_to_frame(target_time)
        start_frame = max(0, target_frame - window_frames // 2)
        end_frame = min(len(energy), target_frame + window_frames // 2)

        window_energy = energy[start_frame:end_frame]
        min_energy_idx = np.argmin(window_energy)
        best_frame = start_frame + min_energy_idx
        best_time = frame_to_time(best_frame)

        cut_points.append(best_time)

    print(f"‚úì {len(cut_points)} —Ç–æ—á–µ–∫ —Ä–∞–∑—Ä–µ–∑–∞–Ω–∏—è –Ω–∞–π–¥–µ–Ω–æ")
    return cut_points


def split_audio_by_cut_points(waveform, sr, cut_points):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ –ø–æ —Ç–æ—á–∫–∞–º"""
    chunks = []
    prev_pos = 0

    for best_time in cut_points:
        cut_sample = int(best_time * sr)
        chunk = waveform[prev_pos:cut_sample]
        if len(chunk) > 0:
            chunks.append(chunk)
        prev_pos = cut_sample

    if prev_pos < len(waveform):
        chunks.append(waveform[prev_pos:])

    return chunks


def prepare_batch_pinned(chunks, start_idx, batch_size, sr):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞—Ç—á –≤ Pinned Memory
    –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç DMA –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ (–±—ã—Å—Ç—Ä–µ–µ —Å CPU –Ω–∞ GPU)
    """
    batch = chunks[start_idx:start_idx+batch_size]

    mel_batch = []
    att_mask_batch = []

    for chunk in batch:
        proc = extractor(chunk, sampling_rate=sr, padding="max_length",
                         max_length=480_000, return_attention_mask=True, return_tensors="pt")
        mel_batch.append(proc.input_features.squeeze(0))
        att_mask_batch.append(proc.attention_mask.squeeze(0))

    mel = torch.stack(mel_batch)
    att_mask = torch.stack(att_mask_batch)

    # ‚úÖ –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –≤ Pinned Memory
    mel = mel.pin_memory()
    att_mask = att_mask.pin_memory()

    return mel, att_mask


def process_chunks_v4(chunks, sr, batch_size=32):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ v4.0

    ‚úÖ Batch Size 32
    ‚úÖ Pinned Memory
    ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
    ‚úÖ GPU + CPU –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """
    results = []
    results_lock = threading.Lock()
    batch_queue = Queue(maxsize=2)
    stop_event = threading.Event()

    def gpu_worker():
        """–ü–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ GPU"""
        try:
            while not stop_event.is_set():
                try:
                    item = batch_queue.get(timeout=1)
                    if item is None:
                        break

                    mel, att_mask, batch_indices = item

                    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º stream
                    with torch.cuda.stream(transfer_stream):
                        mel = mel.to("cuda", non_blocking=True)
                        att_mask = att_mask.to("cuda", non_blocking=True)

                    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º
                    transfer_stream.synchronize()

                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ GPU
                    with torch.inference_mode():
                        transcripts = model.generate(
                            mel=mel, att_mask=att_mask,
                            **generation_params
                        )

                    with results_lock:
                        for idx, transcript in zip(batch_indices, transcripts):
                            results.append((idx, str(transcript)))

                except Exception as e:
                    print(f"GPU worker –æ—à–∏–±–∫–∞: {e}")
        except Exception as e:
            print(f"GPU worker fatal: {e}")

    # –ó–∞–ø—É—Å–∫–∞–µ–º GPU –ø–æ—Ç–æ–∫
    gpu_thread = threading.Thread(target=gpu_worker, daemon=False)
    gpu_thread.start()

    # ========== MAIN LOOP (CPU) ==========
    print(f"\n[–û–ë–†–ê–ë–û–¢–ö–ê] v4.0 - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks)} –∫—É—Å–∫–æ–≤")
    print(f"            Batch Size: 32 | Pinned Memory ‚úì | GPU + CPU –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ ‚ö°")

    for batch_start_idx in range(0, len(chunks), batch_size):
        # CPU –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞—Ç—á –ø–æ–∫–∞ GPU —Ä–∞–±–æ—Ç–∞–µ—Ç
        mel, att_mask = prepare_batch_pinned(chunks, batch_start_idx, batch_size, sr)
        batch_indices = list(range(batch_start_idx, min(batch_start_idx + batch_size, len(chunks))))

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
        batch_queue.put((mel, att_mask, batch_indices))

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        processed = min(batch_start_idx + batch_size, len(chunks))
        progress = (processed / len(chunks)) * 100
        bar_length = 40
        filled = int(bar_length * processed / len(chunks))
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)
        print(f"  [{bar}] {processed}/{len(chunks)} ({progress:.0f}%)", end="\r")

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    batch_queue.put(None)
    gpu_thread.join()

    print()

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    results.sort(key=lambda x: x[0])
    return [text for _, text in results]


def transcribe_interview(audio_file_path, output_file_path="transcript.txt"):
    """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é"""

    total_start = time.time()

    # ============ –ó–ê–ì–†–£–ó–ö–ê ============
    print(f"\n[–ó–ê–ì–†–£–ó–ö–ê] {audio_file_path}...")
    load_start = time.time()

    try:
        waveform, sr = librosa.load(audio_file_path, sr=16_000)
        total_duration = len(waveform) / sr
        load_time = time.time() - load_start

        print(f"‚úì {total_duration:.1f}s –∑–∞ {load_time:.2f}s")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None

    # ============ –ê–ù–ê–õ–ò–ó ============
    analysis_start = time.time()
    cut_points = find_optimal_cut_points(waveform, sr, target_chunk_duration=30)
    chunks = split_audio_by_cut_points(waveform, sr, cut_points)
    analysis_time = time.time() - analysis_start

    print(f"‚úì {len(chunks)} –∫—É—Å–∫–æ–≤ –∑–∞ {analysis_time:.2f}s")

    # ============ –û–ë–†–ê–ë–û–¢–ö–ê ============
    process_start = time.time()
    results = process_chunks_v4(chunks, sr, batch_size=32)  # ‚úÖ Batch 32!
    process_time = time.time() - process_start

    full_transcript = " ".join(results)

    print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞ {process_time:.2f}s")
    print(f"  –°–∫–æ—Ä–æ—Å—Ç—å: {total_duration/process_time:.0f}x —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")

    # ============ –°–û–•–†–ê–ù–ï–ù–ò–ï ============
    print(f"\n[–°–û–•–†–ê–ù–ï–ù–ò–ï]...")
    save_start = time.time()

    try:
        output_path = Path(output_file_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(full_transcript)

        save_time = time.time() - save_start

        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None

    # ============ –ò–¢–û–ì–ò ============
    total_time = time.time() - total_start
    words = len(full_transcript.split())
    chars = len(full_transcript)
    gpu_mem = torch.cuda.memory_allocated() / 1e9

    print(f"\n{'‚ïê' * 90}")
    print(f"[–†–ï–ó–£–õ–¨–¢–ê–¢] v4.0 FULLY OPTIMIZED")
    print(f"{'‚ïê' * 90}")

    print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:")
    print(f"  ‚îú‚îÄ –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ:        {load_time:>6.2f}s")
    print(f"  ‚îú‚îÄ –ê–Ω–∞–ª–∏–∑ —ç–Ω–µ—Ä–≥–∏–∏:        {analysis_time:>6.2f}s")
    print(f"  ‚îú‚îÄ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (v4.0):   {process_time:>6.2f}s ‚ö°")
    print(f"  ‚îú‚îÄ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ:            {save_time:>6.2f}s")
    print(f"  ‚îî‚îÄ –ò–¢–û–ì–û:                 {total_time:>6.2f}s")

    print(f"\nüìä –¢–µ–∫—Å—Ç:")
    print(f"  ‚îú‚îÄ –°–∏–º–≤–æ–ª–æ–≤:  {chars:>10}")
    print(f"  ‚îú‚îÄ –°–ª–æ–≤:      {words:>10}")
    print(f"  ‚îî‚îÄ –°–∫–æ—Ä–æ—Å—Ç—å:  {total_duration/total_time:>9.1f}x –æ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏")

    print(f"\nüíæ –†–µ—Å—É—Ä—Å—ã:")
    print(f"  ‚îú‚îÄ GPU –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {gpu_mem:>5.2f}GB (–∏–∑ 16GB)")
    print(f"  ‚îî‚îÄ GPU —Å–≤–æ–±–æ–¥–Ω–æ:     {16-gpu_mem:>5.2f}GB")

    print(f"\n‚ú® –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ v4.0:")
    print(f"  ‚úì Batch Size 32 (–≤–º–µ—Å—Ç–æ 16)")
    print(f"  ‚úì Pinned Memory (DMA —É—Å–∫–æ—Ä–µ–Ω–∏–µ)")
    print(f"  ‚úì Non-blocking transfer")
    print(f"  ‚úì CUDA Streams –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å")
    print(f"  ‚úì torch.compile (reduce-overhead)")
    print(f"  ‚úì –£–º–Ω–æ–µ —Ä–∞–∑—Ä–µ–∑–∞–Ω–∏–µ –ø–æ –ø–∞—É–∑–∞–º")

    print(f"\nüìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
    print(f"  Baseline (batch16):    9.06s")
    print(f"  v4.0 OPTIMIZED:        {process_time:.2f}s")
    print(f"  –£—Å–∫–æ—Ä–µ–Ω–∏–µ:             {9.06/process_time:.2f}x ‚ö°")

    print(f"\n{'‚ïê' * 90}")
    print(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n")

    return full_transcript


# ============================================
# MAIN
# ============================================
if __name__ == "__main__":

    audio_file = os.path.join(os.path.dirname(__file__), "audio.mp3")
    output_file = "transcript.txt"

    if not os.path.exists(audio_file):
        print(f"\n‚ùå –§–∞–π–ª '{audio_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω!\n")
        exit(1)

    print(f"\n[START] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    result = transcribe_interview(audio_file, output_file)

    if result:
        print(f"[END] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
        exit(1)